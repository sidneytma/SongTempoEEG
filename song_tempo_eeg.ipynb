{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjUIv9CH1jLF"
   },
   "source": [
    "# UC San Diego: Neural Data Science\n",
    "## Estimating Song Tempo from EEG signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qP7cEOOj1jLI"
   },
   "source": [
    "## Permissions\n",
    "\n",
    "Place an `X` in the appropriate bracket below to specify if you would like your group's project to be made available to the public. (Note that student names will be included (but PIDs will be scraped from any groups who include their PIDs).\n",
    "\n",
    "* [x] YES - make available\n",
    "* [  ] NO - keep private"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypKjsIz91jLJ"
   },
   "source": [
    "# Names\n",
    "\n",
    "- Sidney Ma\n",
    "- Dibyesh Sahoo\n",
    "- Ryan Ho\n",
    "- Ishaan Chadha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STdLtZHl1jLJ"
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ac5KccCw1jLJ"
   },
   "source": [
    "We analyzed EEG signals from a group of people listening to naturalistic songs in order to predict the tempo of the song. First, we averaged the EEG signals of participants listening to a single song and applied PCA to extract meaningful data. This approach was based on a previous study that found tempo is related to spikes in the PCA signal. We then applied this method to a new dataset and used smoothing algorithms to enhance the accuracy of tempo calculations by identifying the peaks in the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntFFkMAK1jLJ"
   },
   "source": [
    "<a id='research_question'></a>\n",
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KMIBRkF1jLK"
   },
   "source": [
    "Using only EEG data of participants listening to music, is it possible to accurately predict the tempo of the songs they were listening to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRdy_okz1jLK"
   },
   "source": [
    "<a id='background'></a>\n",
    "\n",
    "# Background & Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tempo describes the speed or pace of a song. We as humans are very good at \"feeling\" the tempo of a song, even when we're not aware of it. Even though audio waveforms of songs are difficult to interpret on their own, it is very easy to hear structure and patterns in them. This alone should suggest that there is some higher-level processing in the brain that can identify tempo and help us \"feel\" the beat (Quinn and Watt 2006). Thus, we should expect to see some indication of this in electrocortical recordings when people listen to music.\n",
    "\n",
    "This is what Kaneshiro et al. found in their 2017 study – after only basic processing of EEG data, they revealed strong oscillations at frequencies related to the tempo and beat structure of a listened song. In 2020, they reinforced this in another study, where this effect occured even in reversed or reshuffled songs, so long as the tempo was the same.\n",
    "\n",
    "However, while these studies show a clear relationship between EEG data and tempo, there is not yet any good way to quantify this relationship, nor is there a way to predict the tempo using only the EEG data. Our goal in this project is to develop a pipeline that can input pre-processed EEG data and output an estimated tempo, without having any prior knowledge of the song. This could expand the field of music information retrieval (MIR) and quantify the ability to retrieve cortical information. Additionally, prediction errors could provide insight as to what makes some songs \"easy\" or \"hard\" to follow along to. In general, it is a tool that will help us better understand how auditory information is processed by humans, and how raw signals can be decoded into meaningful structures.\n",
    "\n",
    "References:\n",
    "- https://pubmed.ncbi.nlm.nih.gov/16583770/\n",
    "- https://ccrma.stanford.edu/~blairbo/assets/pdf/losorelli2017ISMIR.pdf\n",
    "- https://www.sciencedirect.com/science/article/pii/S105381192030046X?via%3Dihub\n",
    "\n",
    "### About tempo\n",
    "\n",
    "Tempo is typically defined in beats per minute (bpm), but it can also be defined in Hz (beats per second), a conversion factor of 60. When listeners feel a tempo, they also feel tempo-related frequencies, which are typically in a power series of 2. Specifically, given tempo $T$, tempo-related frequencies are $T*2^N $ where $N$ is any integer. As Kaneshiro et al. found, oscillations spike at tempo-related frequencies, not just the tempo frequency. As a result, it is very difficult to determine which tempo-related frequency is the actual tempo.\n",
    "\n",
    "For example, if a song has a tempo of 160 bpm (2.7 Hz), then 80 bpm (1.3 Hz) is a tempo-related frequency, and strong spikes would likely show up at both frequencies. From these spikes alone, it would be basically impossible to know whether the song was 160 or 80 bpm (even humans might disagree about the tempo!). For this reason, we are choosing to normalize the tempo estimates to be between 60 and 120 bpm (1 and 2 Hz). Any tempo has exactly one tempo-related frequency in this range. For the purpose of our project, if a song has a tempo outside of this range, we will simply choose the tempo-related frequency that is inside this range, and interpret that to be the true tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rymPn0nC1jLL"
   },
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj6wP3DB1jLL"
   },
   "source": [
    "Given the relationship between oscillation spikes and tempo, we expect that it is possible to estimate the tempo accurately. However, we will not be able to account for octave differences (i.e. our algorithm will not distinguish tempo $T$ from tempo $T*2^N$), and our tool will only provide estimates between 60 and 120 bpm (1 and 2 Hz)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32Zmp0PI1jLL"
   },
   "source": [
    "# Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhWXEgT61jLL"
   },
   "source": [
    "- Dataset Name: Naturalistic Music EEG Dataset - Tempo (NMED-T)\n",
    "- Link to the dataset: https://purl.stanford.edu/jn859kj8079\n",
    "- Number of observations: 10\n",
    "\n",
    "This dataset contains preprocessed EEG data collected from participants while they listened to 10 different songs. Each observation is a 125 x 34795 x 20 matrix, representing the electrodes, timepoints, and participants, respectively. It uses a sample rate of 125, meaning each recording was about 4 minutes 28 seconds long.\n",
    "\n",
    "- Dataset Name: Naturalistic Music EEG Dataset - Hindi (NMED-H)\n",
    "- Link to the dataset: https://purl.stanford.edu/sd922db3535\n",
    "- Number of observations: 8\n",
    "\n",
    "This dataset contains preprocessed EEG data collected from participants while they listened to 4 different Hindi songs; there are 8 observations because there were two listening sessions (denoted *a* and *b*) for each song. Each observation is a 125 x 33271 x 12 matrix, representing the electrodes, timepoints, and participants, respectively. It also uses a sample rate of 125, meaning each recording was about 4 minutes 26 seconds long.\n",
    "\n",
    "The preprocessing is the same for both datasets, and is described in the corresponding papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gI1hvLW41jLM"
   },
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.io\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "from tqdm import tqdm\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We downloaded the EEG data directly from the dataset web pages into a personal computer. In total, this amounted to 10 GB. They are matricies are stored as .mat files, with the names listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames for NMED-T\n",
    "data_t_filenames = ['song21_Imputed.mat',\n",
    " 'song22_Imputed.mat',\n",
    " 'song23_Imputed.mat',\n",
    " 'song24_Imputed.mat',\n",
    " 'song25_Imputed.mat',\n",
    " 'song26_Imputed.mat',\n",
    " 'song27_Imputed.mat',\n",
    " 'song28_Imputed.mat',\n",
    " 'song29_Imputed.mat',\n",
    " 'song30_Imputed.mat']\n",
    "\n",
    "# Filenames for NMED-H\n",
    "data_h_filenames = ['song21_a_Imputed.mat',\n",
    " 'song21_b_Imputed.mat',\n",
    " 'song22_a_Imputed.mat',\n",
    " 'song22_b_Imputed.mat',\n",
    " 'song23_a_Imputed.mat',\n",
    " 'song23_b_Imputed.mat',\n",
    " 'song24_a_Imputed.mat',\n",
    " 'song24_b_Imputed.mat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Processing\n",
    "\n",
    "Since none of us own MATLAB, we needed to pull the files in as ndarrays in order to work with them in python. However, if we were to pull in all 14 matricies at once, this would not only use up a lot of memory, but would also take way too long to process. Instead, we pulled in each matrix and ran some initial processing (including dimensionality reduction) before moving onto the next.\n",
    "\n",
    "Our process replicates the one described in Kaneshiro et al. (2017), which involves these three steps:\n",
    "1. Compute the mean signal across participants\n",
    "2. Compute the first pricipal component (PC1) of the mean signal\n",
    "3. Run a Fast Fourier Transform (FFT) on the PC1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dIwAKR8F1jLM"
   },
   "outputs": [],
   "source": [
    "sr = 125 # Sample rate is 125 for both datasets\n",
    "def collect_pc1_data(eeg, output_name):\n",
    "    # Mean eeg signal across participants\n",
    "    mean_eeg = np.mean(eeg, axis=2)\n",
    "    mean_eeg_flat = mean_eeg.reshape(125, -1)\n",
    "\n",
    "    # PC1\n",
    "    U, S, Vt = np.linalg.svd(mean_eeg_flat, full_matrices=False)\n",
    "    PC1 = U[:, 0]\n",
    "    PC1_time_series = PC1.dot(mean_eeg_flat)\n",
    "\n",
    "    # FFT\n",
    "    N = PC1_time_series.shape[0]\n",
    "    yf = fft(PC1_time_series)\n",
    "    xf = fftfreq(N, 1/sr)\n",
    "    freq, power = xf[:N//2], np.abs(yf[:N//2])\n",
    "    pc1_data = np.vstack((freq, power))\n",
    "    \n",
    "    np.save(output_name, pc1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saved each reduced dataset into the folders pc1-T (for NMED-T) and pc1-H (for NMED-H). Note that for NMED-H, it vertically stacks the a and b matricies, effectively doubling the number of participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(10)):\n",
    "    source_name = f\"data-T/{data_t_filenames[i]}\"\n",
    "    column_name = f\"data{i + 21}\"\n",
    "    \n",
    "    mat = scipy.io.loadmat(source_name)\n",
    "    eeg = mat[column_name]\n",
    "    \n",
    "    collect_pc1_data(eeg, f\"pc1-T/t{i+1}\")\n",
    "\n",
    "for i in tqdm(range(4)):\n",
    "    source_a_name = f\"data-H/song{i + 21}_a_Imputed.mat\"\n",
    "    source_b_name = f\"data-H/song{i + 21}_b_Imputed.mat\"\n",
    "    column_a_name = f\"data{i + 21}_a\"\n",
    "    column_b_name = f\"data{i + 21}_b\"\n",
    "    \n",
    "    mat_a = scipy.io.loadmat(source_a_name)\n",
    "    eeg_a = mat_a[column_a_name]\n",
    "    \n",
    "    mat_b = scipy.io.loadmat(source_b_name)\n",
    "    eeg_b = mat_b[column_b_name]\n",
    "    \n",
    "    eeg = np.vstack((eeg_a, eeg_b)) # treat A and B entries like different participants\n",
    "    \n",
    "    collect_pc1_data(eeg, f\"pc1-H/h{i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaNHX_Ri1jLN"
   },
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial processing explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, to illustrate the above process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/p5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our process involved computing the mean signal, the PC1, and the FFT of each EEG matrix. While we could have simply computed the FFT of the mean signal (top right), this does not display any prominent spikes, which is necessary for predicting the tempo.\n",
    "\n",
    "## Observing tempo lineups for NMED-H\n",
    "\n",
    "Next, we replicated the plot in Kaneshiro et al. (2017), using the four Hindi songs instead:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/p1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows the FFT signals for the four Hindi songs. Using the tempos provided in Kaneshiro et al. (2020), we plotted vertical lines at tempo-related frequencies, that is $T*2^N$ for $N$ in range $[-3, 3]$. As expected, the peaks in the FFT signals did seem to line up with tempo-related frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pDzM8f71jLN"
   },
   "source": [
    "# Data Analysis\n",
    "While it is clear that the FFT signals are related to the tempo, it is not immediately clear how they can be used to predict the tempo. We devised three high-level steps to accomplish this:\n",
    "1. Log the frequency spectrum so that the peaks are more spread out\n",
    "2. Find the frequency positions of the \"prominent\" peaks\n",
    "3. Use these frequency positions to estimate the tempo\n",
    "\n",
    "### Step 1: Log frequency\n",
    "\n",
    "Both the peaks and the tempo-related frequencies are squished together at low frequencies, so it is necessary to view the frequency spectrum on a log scale in order to identify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_freq(freq, power):\n",
    "    freq = np.where(freq == 0, freq + 0.00001, freq) # avoid log(0) issues\n",
    "    freq = np.log2(freq)\n",
    "    mask = (freq >= -3) & (freq <= 5) # only include positions between 1/8 Hz and 32 Hz\n",
    "    freq = freq[mask]\n",
    "    power = power[mask]\n",
    "    \n",
    "    return freq, power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/p2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Find peaks\n",
    "The goal here is to locate the frequency positions of \"prominent\" peaks. There are a variety of challenges within this step. Firstly, the FFT signals are basically entirely composed of peaks, so it is difficult to identify the \"prominent\" ones from the \"normal\" ones. Secondly, the FFTs all follow a non-linear trend, which means that the peaks are relative, not absolute. Thirdly, because the frequency axis is logscaled, peaks closer to 0 Hz will be much wider than those at higher frequencies.\n",
    "\n",
    "To accomplish our goal, we implemented a variety of simple statistical methods to emphasize the \"prominent\" peaks and dampen the \"normal\" ones:\n",
    "- Moving maximum: a windowing function that returns the maximum value of the window. This effectively \"connects the dots,\" tracing out large peaks and ignores smaller ones.\n",
    "- Exclusive moving average: a windowing function that returns the average value of the window, excluding the current point. This accounts for the non-linear trend, setting the peaks onto an \"even playing field.\" By excluding the current point, it emphasizes any \"sudden\" differences that appear at peaks.\n",
    "- 1D Gaussian filter: a smoothing function that helps us identify peaks by finding the local maxima.\n",
    "\n",
    "The pipeline is described in code and illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peaks(freq, power):\n",
    "    A = mov_max(power, 10)\n",
    "    B = mov_avg_exc(A, 100)\n",
    "    C = heavyside(A - B)\n",
    "    D = mov_max(C, 20)**2\n",
    "    curve = gaussian_filter1d(D, 5)\n",
    "    \n",
    "    # Find peaks\n",
    "    peaks = np.where((curve[1:-1] > curve[:-2]) & (curve[1:-1] > curve[2:]))[0] + 1\n",
    "    \n",
    "    return curve, peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/p3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were only able to decide on the parameters for the function through experimenting with different values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Estimating tempo\n",
    "Now that the peaks are identified, the next challenge is to estimate the tempo. There are three sub-tasks involved in this step:\n",
    "1. Finding the frequencies of the largest peaks\n",
    "2. Normalizing the frequencies\n",
    "3. Deciding which frequency is most likely to be the tempo\n",
    "\n",
    "Finding the frequencies of the largest peaks is somewhat difficult, but only involves dataframe sorting. After some experimenting, we decided that the top 6 largest peaks were most representative of the tempo – anything lower was likely to be random.\n",
    "\n",
    "Normalizing the frequencies means selecting the tempo-related frequency between 1 and 2 Hz. To do this, we must multiply or divide each frequency by 2 until it is between 1 and 2 Hz. In log space, this is simply finding the decimal value of the log frequency. \n",
    "\n",
    "Deciding on the correct frequency is difficult – of the 6 selected frequencies, typically only 3 or 4 are actually close the the correct tempo. Central tendency measures like mean or median don't return good results, as they are overly influenced by bad estimates. After some experimentation, we decided on using the medioid as the best estimate. This way, good estimates would be much more likely than bad estimates to be the final guess.\n",
    "\n",
    "Importantly, due to the cyclical nature of tempo, the similarity of two tempos should be quantified as the circular distance between them, not the arithmetic difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_tempo(freq, curve, peaks):\n",
    "    # Find frequencies of largest peaks\n",
    "    df = pd.DataFrame((freq[peaks], curve[peaks])).T\n",
    "    df.columns = \"freq\", \"power\"\n",
    "    df_sort = df.sort_values(by=\"power\", ascending=False).head(6)\n",
    "    peaks_sorted = df_sort[\"freq\"]\n",
    "    \n",
    "    # Normalize the frequencies\n",
    "    peaks_normalized = (peaks_sorted % 1).to_list()\n",
    "    \n",
    "    # Decide which frequency is most likely\n",
    "    similarity_matrix = [[circ_diff(a, b) for b in peaks_normalized] for a in peaks_normalized]\n",
    "    similarity_matrix = np.array(similarity_matrix)\n",
    "    most_accurate_idx = np.argmin(similarity_matrix.sum(axis=0))\n",
    "    medioid = peaks_normalized[most_accurate_idx]\n",
    "    \n",
    "    return medioid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the PC1 data, output the tempo guess\n",
    "def estimate_tempo_from_pca(freq, power):\n",
    "    log_freq, power_mask = get_log_freq(freq, power)\n",
    "    curve, peaks = get_peaks(log_freq, power_mask)\n",
    "    return estimate_tempo(log_freq, curve, peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate tempos for NMED-T\n",
    "estimated_tempos_t = []\n",
    "for i in range(10):\n",
    "    filename = f\"pc1-T/t{i+1}.npy\"\n",
    "    freq, power = np.load(filename)\n",
    "    estimated_tempos_t.append(estimate_tempo_from_pca(freq, power))\n",
    "    \n",
    "# Estimate tempos for NMED-H\n",
    "estimated_tempos_h = []\n",
    "for i in range(4):\n",
    "    filename = f\"pc1-H/h{i+1}.npy\"\n",
    "    freq, power = np.load(filename)\n",
    "    estimated_tempos_h.append(estimate_tempo_from_pca(freq, power))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Using the true tempo values provided in both Kaneshiro et al. studies, we evaluated the predicted tempos. The results are displayed in the following table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error: 0.0057879864287789795\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hz</th>\n",
       "      <th>bpm</th>\n",
       "      <th>log_hz</th>\n",
       "      <th>hz_norm</th>\n",
       "      <th>hz_estimated</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9328</td>\n",
       "      <td>55.968</td>\n",
       "      <td>-0.100360</td>\n",
       "      <td>0.899640</td>\n",
       "      <td>0.873545</td>\n",
       "      <td>0.026094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.1574</td>\n",
       "      <td>69.444</td>\n",
       "      <td>0.210888</td>\n",
       "      <td>0.210888</td>\n",
       "      <td>0.220950</td>\n",
       "      <td>0.010063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2376</td>\n",
       "      <td>74.256</td>\n",
       "      <td>0.307545</td>\n",
       "      <td>0.307545</td>\n",
       "      <td>0.306400</td>\n",
       "      <td>0.001145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.3736</td>\n",
       "      <td>82.416</td>\n",
       "      <td>0.457962</td>\n",
       "      <td>0.457962</td>\n",
       "      <td>0.442340</td>\n",
       "      <td>0.015622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.5244</td>\n",
       "      <td>91.464</td>\n",
       "      <td>0.608242</td>\n",
       "      <td>0.608242</td>\n",
       "      <td>0.616677</td>\n",
       "      <td>0.008435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.6026</td>\n",
       "      <td>96.156</td>\n",
       "      <td>0.680414</td>\n",
       "      <td>0.680414</td>\n",
       "      <td>0.677031</td>\n",
       "      <td>0.003384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.8116</td>\n",
       "      <td>108.696</td>\n",
       "      <td>0.857264</td>\n",
       "      <td>0.857264</td>\n",
       "      <td>0.861163</td>\n",
       "      <td>0.003898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0000</td>\n",
       "      <td>120.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.1368</td>\n",
       "      <td>128.208</td>\n",
       "      <td>1.095452</td>\n",
       "      <td>0.095452</td>\n",
       "      <td>0.101028</td>\n",
       "      <td>0.005576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.5000</td>\n",
       "      <td>150.000</td>\n",
       "      <td>1.321928</td>\n",
       "      <td>0.321928</td>\n",
       "      <td>0.322083</td>\n",
       "      <td>0.000155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.6000</td>\n",
       "      <td>156.000</td>\n",
       "      <td>1.378512</td>\n",
       "      <td>0.378512</td>\n",
       "      <td>0.378435</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.5600</td>\n",
       "      <td>93.600</td>\n",
       "      <td>0.641546</td>\n",
       "      <td>0.641546</td>\n",
       "      <td>0.647546</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.5000</td>\n",
       "      <td>90.000</td>\n",
       "      <td>0.584963</td>\n",
       "      <td>0.584963</td>\n",
       "      <td>0.348428</td>\n",
       "      <td>0.236534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.4300</td>\n",
       "      <td>85.800</td>\n",
       "      <td>0.516015</td>\n",
       "      <td>0.516015</td>\n",
       "      <td>0.531687</td>\n",
       "      <td>0.015671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        hz      bpm    log_hz   hz_norm  hz_estimated     error\n",
       "0   0.9328   55.968 -0.100360  0.899640      0.873545  0.026094\n",
       "1   1.1574   69.444  0.210888  0.210888      0.220950  0.010063\n",
       "2   1.2376   74.256  0.307545  0.307545      0.306400  0.001145\n",
       "3   1.3736   82.416  0.457962  0.457962      0.442340  0.015622\n",
       "4   1.5244   91.464  0.608242  0.608242      0.616677  0.008435\n",
       "5   1.6026   96.156  0.680414  0.680414      0.677031  0.003384\n",
       "6   1.8116  108.696  0.857264  0.857264      0.861163  0.003898\n",
       "7   2.0000  120.000  1.000000  0.000000      0.000104  0.000104\n",
       "8   2.1368  128.208  1.095452  0.095452      0.101028  0.005576\n",
       "9   2.5000  150.000  1.321928  0.321928      0.322083  0.000155\n",
       "10  2.6000  156.000  1.378512  0.378512      0.378435  0.000077\n",
       "11  1.5600   93.600  0.641546  0.641546      0.647546  0.006000\n",
       "12  1.5000   90.000  0.584963  0.584963      0.348428  0.236534\n",
       "13  1.4300   85.800  0.516015  0.516015      0.531687  0.015671"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempos_t = [0.9328, 1.1574, 1.2376, 1.3736, 1.5244, 1.6026, 1.8116, 2.0000, 2.1368, 2.5000]\n",
    "tempos_h = [2.60, 1.56, 1.50, 1.43]\n",
    "\n",
    "df = pd.DataFrame(tempos_t + tempos_h, columns = [\"hz\"]) # true tempos\n",
    "df[\"bpm\"] = df[\"hz\"] * 60 # in bpm\n",
    "df[\"log_hz\"] = df[\"hz\"].apply(np.log2) # logged\n",
    "df[\"hz_norm\"] = df[\"log_hz\"] % 1 # normalized between 1 and 2 hz\n",
    "\n",
    "df[\"hz_estimated\"] = estimated_tempos_t + estimated_tempos_h # estimated tempos\n",
    "df[\"error\"] = df.apply( # error using circular difference\n",
    "    lambda row: circ_diff(row[\"hz_norm\"], row[\"hz_estimated\"]), axis=1)\n",
    "\n",
    "# Display results\n",
    "print(\"Mean error:\", df[\"error\"].median())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/p4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the table and plot, our estimated tempos were very accurate. Only one song had an error ratio larger than 3%, and the median error ratio was 0.58%.\n",
    "\n",
    "The outlier song (Song 12) is \"Haule Haule\" from \"Rab Ne Bana Di Jodi\" (2008). Looking at the following graph, it is understandable why the algorithm failed on this one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/p6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the tempo is lined up with two of the peaks, but misses the ones in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0bJprCC1jLN"
   },
   "source": [
    "# Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOkpBMiJ1jLN"
   },
   "source": [
    "Although tempo can be hard to interpret from a song's waveform, it is very easy, even automatic, for humans to find. This is only further evidenced by our results, which indicate that a song's tempo is not only linked to the corresponding EEG data, but can even be predicted by it.\n",
    "\n",
    "The main limitation our project faces is that it cannot predict any tempos outside of its specified range (60 - 120 bpm). While this would be difficult to do, there should still be certain artifacts in the FFT signal that could indicate whether or not the tempo is outside of this range. Our tool will also not work with uneven time signatures, as we only consider temporal structure with powers of two. Apart from that, we have not tested the external validity of our project, which is important considering that both datasets come from the same researchers, and likely the same equipment.\n",
    "\n",
    "The failure to identify the correct tempo for Song 12 shows that many of the peaks are not at tempo-related frequencies, or at least not in the tempo-related power series. Future work could investigate what frequencies these peaks lie on and how, if at all, they relate to the main tempo.\n",
    "\n",
    "Overall, we are happy with the results, and hope to see similar work in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
